
## 决策树的生成是一个递归过程.在决策树基本算法中，有三种情形会导致递归返回:
###  (1) 当前结点包含的样本全属于同一类别，无需划分;
###  (2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分; 
###  (3) 当前结点包含的样本集合为空，不能划分.

在第(2)种情形下?我们把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别;在第(3) 种情形下，同样把当前结点标记为叶结点) 1且将其类别设定为其父结点所含样本最多的类别.注意这两种情形的处理实质不同:情形(2)是在利用当前结点的后验分布，而情形(3)则是把父结点的样本分布作为当前结点的先捡分布.
## 划分选择:[我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的"纯度" (purity) 越来越高.]
ent(D) 的值越小，则D 的纯度越高.

**ID3 决策树学习算法[Quinlan ， 1986] 就是以信息增益为准则来选择划分属性** 

在决策树学习开始时，根结点包含D 中的所有样例:
根结点的信息:ent(D)

然后，我们要计算出当前属性集合中每个属性的信息增益；
ent(D)  -   Ent(Dl)
ent(D)  -   Ent(D2)
ent(D)  -   Ent(D3)
.......
计算各个分支的信息熵，与根节点的熵进行比较，获得所求的信息增益。

其中信息增益最大的被选为划分属性；多个属性均取得了最大的信息增益，可任选其中之一作为划分属性。  

**著名的C4.5 决策树算法[Quinlan， 1993J 不直接使(gain用信息增益，而是使用"增益率"ratio) 来选择最优划分属性.**   

