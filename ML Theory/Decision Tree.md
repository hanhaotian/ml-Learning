
## 决策树的生成是一个递归过程.在决策树基本算法中，有三种情形会导致递归返回:
###  (1) 当前结点包含的样本全属于同一类别，无需划分;
###  (2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分; 
###  (3) 当前结点包含的样本集合为空，不能划分.

在第(2)种情形下?我们把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别;在第(3) 种情形下，同样把当前结点标记为叶结点) 1且将其类别设定为其父结点所含样本最多的类别.注意这两种情形的处理实质不同:情形(2)是在利用当前结点的后验分布，而情形(3)则是把父结点的样本分布作为当前结点的先捡分布.
## 划分选择:[我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的"纯度" (purity) 越来越高.]
ent(D) 的值越小，则D 的纯度越高.

**ID3 决策树学习算法[Quinlan ， 1986] 就是以信息增益为准则来选择划分属性** 

在决策树学习开始时，根结点包含D 中的所有样例:
根结点的信息:ent(D)

然后，我们要计算出当前属性集合中每个属性的信息增益；
ent(D)  -   Ent(Dl)
ent(D)  -   Ent(D2)
ent(D)  -   Ent(D3)
.......
计算各个分支的信息熵，与根节点的熵进行比较，获得所求的信息增益。

其中信息增益最大的被选为划分属性；多个属性均取得了最大的信息增益，可任选其中之一作为划分属性。  

**著名的C4.5 决策树算法[Quinlan， 1993J 不直接使(gain用信息增益，而是使用"增益率"ratio) 来选择最优划分属性.**   

C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式[Quinlan， 1993]: 先从候选划分属性中找出信息增益高于平均水平的属性，再从
中选择增益率最高的.

**CART 决策树[Breiman et al., 1984] 使用"基尼指数" (Gini index)来选择划分属性.**  
[CART 是Classification and Regression Tr.胆的简称，这是一种著名的决策树学习算法，分类和回归任务都可用]  
直观来说， Gini(D) 反映了从数据集D 中随机抽取两个样本，其类别标记不一致的概率.因此， Gini(D) 越小，则数据集D 的纯度越高.  
于是，我们在候选属性集合A 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即向= argmin GiniJndex(D ， α).


## 剪枝处理
在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得"太好"了，以致于把训练集自身
的一些特点当作所有数据都具有的一般性质而导致过拟合.因此，可通过主动去掉一些分支来降低过拟合的风险.  
[剪枝(pruning)是决策树学习算法对付"过拟合"的主要手段]  


基本策略有"预剪枝" (prepruning)和"后剪枝"(post"pruning) [Quinlan, 1993].   

**预剪枝是指:** 在决策树生成过程中，对每个结点在划
分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划
分并将当前结点标记为叶结点;
**后剪枝则是:** 先从训练集生成一棵完整的决策树，
然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.

## 连续与缺失值  
基于划分点t 可将D 分为于集D-和D+? 其中D- 包含那些在属性α 上取值不大于t 的样本时D+ 则包含那些在属性α 上取值大于t 的样本
***【与离散属性不同，若当前结点划分属性为连续属性?该属性还可作为其后代结点的划分属性.】
















## 概述

一、简介
        决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测  
二、基本思想
### 1）树以代表训练样本的单个结点开始。
### 2）如果样本都在同一个类．则该结点成为树叶，并用该类标记。
### 3）否则，算法选择最有分类能力的属性作为决策树的当前结点．###
### 4）根据当前决策结点属性取值的不同，将训练样本数据集tlI分为若干子集，每个取值形成一个分枝，有几个取值形成几个分枝。匀针对上一步得到的一个子集，重复进行先前  步骤，递4'I形成每个划分样本上的决策树。一旦一个属性出现在一个结点上，就不必在该结点的任何后代考虑它。
### 5）递归划分步骤仅当下列条件之一成立时停止：
       ①给定结点的所有样本属于同一类。
       ②没有剩余属性可以用来进一步划分样本．在这种情况下．使用多数表决，将给定的结点转换成树叶，并以样本中元组个数最多的类别作为类别标记，同时也可以存放该结点样本的类别分布，
       ③如果某一分枝tc，没有满足该分支中已有分类的样本，则以样本的多数类创建一个树叶。  
三、构造方法  
       决策树构造的输入是一组带有类别标记的例子，构造的结果是一棵二叉树或多叉树。二叉树的内部节点(非叶子节点)一般表示为一个逻辑判断，如形式为a=aj的逻辑判断，其中a是属性，aj是该属性的所有取值：树的边是逻辑判断的分支结果。多叉树(ID3)的内部结点是属性，边是该属性的所有取值，有几个属性值就有几条边。树的叶子节点都是类别标记。  
由于数据表示不当、有噪声或者由于决策树生成时产生重复的子树等原因，都会造成产生的决策树过大。因此，简化决策树是一个不可缺少的环节。寻找一棵最优决策树，主要应解决以下3个最优化问题：  
①生成最少数目的叶子节点；  
②生成的每个叶子节点的深度最小；  
③生成的决策树叶子节点最少且每个叶子节点的深度最小。  
