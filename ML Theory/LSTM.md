# LSTM学习笔记


## 什么是LSTM和BiLSTM？  
LSTM的全称是Long Short-Term Memory，它是RNN（Recurrent Neural Network）的一种。LSTM由于其设计的特点，非常适合用于对时序数据的建模，如文本数据。BiLSTM是Bi-directional Long Short-Term Memory的缩写，是由前向LSTM与后向LSTM组合而成。两者在自然语言处理任务中都常被用来建模上下文信息。  

## 为什么使用LSTM与BiLSTM？  
将词的表示组合成句子的表示，可以采用相加的方法，即将所有词的表示进行加和，或者取平均等方法，但是这些方法没有考虑到词语在句子中前后顺序。如句子“我不觉得他好”。“不”字是对后面“好”的否定，即该句子的情感极性是贬义。使用LSTM模型可以更好的捕捉到较长距离的依赖关系。因为LSTM通过训练过程可以学到记忆哪些信息和遗忘哪些信息。

但是利用LSTM对句子进行建模还存在一个问题：无法编码从后到前的信息。在更细粒度的分类时，如对于强程度的褒义、弱程度的褒义、中性、弱程度的贬义、强程度的贬义的五分类任务需要注意情感词、程度词、否定词之间的交互。举一个例子，“这个餐厅脏得不行，没有隔壁好”，这里的“不行”是对“脏”的程度的一种修饰，通过BiLSTM可以更好的捕捉双向的语义依赖。  
  
总体框架：
![avatar](/pic/1.jpg)  

![avatar](/pic/2.jpg)  

主要参数[t时刻下]：  
  
*    输入词X(t) 
*    细胞状态C(t)
*    临时细胞状态~C(t)
*    隐藏状态h(t)
*    遗忘门f(t)
*    记忆门i(t)
*    输出门o(t)

  
## 详细介绍计算过程  

*   **1 计算遗忘门，选择要遗忘的信息**

输入:  前一时刻的隐层状态h(t-1),当前时刻的输入词X(t)
输出:  遗忘门的值f(t)  

![avatar](/pic/3.jpg)  
  
*   **2  计算记忆门，选择要记忆的信息**  
输入:  前一时刻的隐层状态h(t-1),当前时刻的输入词X(t)  
输出:  记忆门的值i(t),临时细胞状态~C(t)  

![avatar](/pic/4.jpg)  
  
*   **3  计算当前时刻细胞状态**    

输入:  记忆门的值i(t),遗忘门的值f(t),临时细胞状态~C(t),上一刻细胞状态C(t-1)
输出:  当前时刻细胞状态C(t)  
![avatar](/pic/5.jpg)  

*   **4  计算输出门和当前时刻隐层状态**    

输入:  前一时刻的隐层状态h(t-1),当前时刻的输入词X(t),当前时刻细胞状态 C(t)
输出:  输出门的值O(t),隐层状态h(t) 
![avatar](/pic/6.jpg)  



最终，我们可以得到与句子长度相同的隐层状态序列{h(0),h(1),......,h(n-1)}

##  BiLSTM介绍
前向的LSTM与后向的LSTM结合成BiLSTM。比如，我们对“我爱中国”这句话进行编码，模型如图所示。  
![avatar](/pic/7.jpg)  

对于情感分类任务来说，我们采用的句子的表示往往是[ h(L2),h(R2)]
![avatar](/pic/8.jpg)  
因为其包含了前向与后向的所有信息，如图所示。  
![avatar](/pic/9.jpg)  
