## 集成学习  
**集成学习的一般结构:**  先产生一组"个体学习
器" (individual learner) ，再用某种策略将它们结合起来.个体学习器通常
由一个现有的学习算法从训练数据产生，例如C4.5 决策树算法、BP 神经网
络算法等，此时集成中只包含同种类型的个体学习器，例如"决策树集成"
中全是决策树?"神经网络集成"中全是神经网络，这样的集成是"同质"
的(homogeneous) .同质集成中的个体学习器亦称"基学习器" (base learner),
相应的学习算法称为"基学习算法" (base learning algorithm). 集成也可包含
不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是"异
质"的(heterogenous) .异质集成中的个体学习器由不同的学习算法生成，这时
就不再有基学习算法;相应的，个体学习器一般不称为基学习器，常称为"组件
学习器" (component learner) 或直接称为个体学习器.  

***
**目前的集成学习方法大致可分为两大类**：  

1.个体学习器问存在强依赖关系、必须串行生成的序列化方法；eg：**Boosting**  
2.个体学习器间不存在强依赖关系、可同时生成的并行化方法；eg：**Bagging** 和**Random Forest**.  
***注：集成学习框架中的基模型是弱模型,通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。
但是，并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。***  
*正由于bagging的训练过程旨在降低方差，而boosting的训练过程旨在降低偏差，过程影响类的参数能够引起整体模型性能的大幅度变化。一般来说，在此前提下，我们继续微调子模型影响类的参数，从而进一步提高模型的性能。*

**学习器结合可能会从三个方面带来好处:**
+ ***从统计的方面来看***，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减小这一风险;
+ ***从计算的方面来看***，学习算法往往会陷入局部极小,有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险;
+ ***从表示的方面来看***，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器，由于相应的假设空间有所扩大，有可能学得更好的近似。  
*结合策略:*
+ 1. 平均法
  + a.简单平均法
  + b.加权平均法
+ 2. 投票法
  + a.绝对多数投票法:即若某标记得票过半数，则预测为该标记;否则拒绝预测.
  + b.相对多数投票法:即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个
  + c.加权投票法
+ 3. 学习法
  
  
